{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sequences\n",
       "0  MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...\n",
       "1  MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...\n",
       "2  MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...\n",
       "3  MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...\n",
       "4  MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_df = pd.read_table('data/family_classification_sequences.tab')\n",
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_codones(sseq):\n",
    "    crop = len(sseq) % 3\n",
    "    cropped_seq = sseq[:-crop] if crop > 0 else sseq\n",
    "\n",
    "    return [cropped_seq[i:i+3] for i in range(0, len(cropped_seq), 3)]\n",
    "\n",
    "def seq_to3(seq):\n",
    "    splittings = [make_codones(seq[i:]) for i in range(3)]\n",
    "    return splittings\n",
    "\n",
    "def create_all_codones(df):\n",
    "    codones = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i, :][0]\n",
    "        codones.extend(seq_to3(row))\n",
    "\n",
    "    return codones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_or_create(read_path, producer):\n",
    "    if os.path.isfile(read_path):\n",
    "        print('reading', read_path)\n",
    "        with open(read_path, 'rb') as fp:\n",
    "            return pickle.load(fp)\n",
    "    result = producer()\n",
    "    print('saving', read_path)\n",
    "    with open(read_path, 'wb') as fp:\n",
    "        pickle.dump(result, fp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data/all_codones.pickle\n"
     ]
    }
   ],
   "source": [
    "all_codones = read_or_create(read_path='data/all_codones.pickle',\n",
    "                             producer= lambda: create_all_codones(seq_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(index_words_list, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    \n",
    "    size = len(index_words_list)\n",
    "    ran = list(range(size))\n",
    "    random.shuffle(ran)\n",
    "    \n",
    "#     for index_words in index_words_list:\n",
    "#         for index, center in enumerate(index_words):\n",
    "\n",
    "    for i in ran:\n",
    "        for index, center in enumerate(index_words_list[i]):\n",
    "            context = random.randint(1, context_window_size)\n",
    "            \n",
    "            # get a random target before the center word\n",
    "#             for target in index_words[max(0, index - context): index]:\n",
    "            for target in index_words_list[i][max(0, index - context): index]:\n",
    "                yield center, target\n",
    "            \n",
    "            # get a random target after the center wrod\n",
    "#             for target in index_words[index + 1: index + context + 1]:\n",
    "            for target in index_words_list[i][index + 1: index + context + 1]:\n",
    "                yield center, target\n",
    "\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        \n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        \n",
    "        yield center_batch, target_batch\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "\n",
    "def cod_to_dict(cod, dictionary):\n",
    "    return [dictionary[key] for key in cod]\n",
    "\n",
    "\n",
    "def make_dictionary(all_codones):\n",
    "    flat_codones = flatten(all_codones)\n",
    "#     unique_codones = set(flat_codones)\n",
    "    \n",
    "    counter = Counter(flat_codones)\n",
    "    \n",
    "    unique_codones = list(set(flat_codones))\n",
    "    unique_codones = sorted(unique_codones, key=lambda x: counter[x], \n",
    "                            reverse=True)\n",
    "    \n",
    "#     print(unique_codones[:6])\n",
    "    \n",
    "#     count = 0\n",
    "\n",
    "#     for key in unique_codones:\n",
    "#         print(key, counter[key])\n",
    "#         count += 1\n",
    "#         if count > 6:\n",
    "#             break\n",
    "            \n",
    "    dictionary = {cod: i for i, cod in enumerate(unique_codones)}\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def process_data(all_codones, dictionary, batch_size, skip_window):\n",
    "    cod_dicts = [cod_to_dict(cod, dictionary) for cod in all_codones]\n",
    "    single_gen = generate_sample(cod_dicts, context_window_size=skip_window)\n",
    "    batch_gen = get_batch(single_gen, batch_size=batch_size)\n",
    "    return batch_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9424\n"
     ]
    }
   ],
   "source": [
    "# all_codones = np.array([[['abc', 'gec', 'agc'], ['get', 'ghe', 'acg']]])\n",
    "\n",
    "dictionary = make_dictionary(all_codones)\n",
    "\n",
    "print(len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SKIP_WINDOW = 10  # the context window\n",
    "\n",
    "batch_gen = process_data(all_codones, dictionary, BATCH_SIZE, SKIP_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data/all_acid_dicts.pickle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acid</th>\n",
       "      <th>hydrophobicity</th>\n",
       "      <th>mass</th>\n",
       "      <th>number_of_atoms</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RQG</td>\n",
       "      <td>-2.800000</td>\n",
       "      <td>113.788733</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>125.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPL</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>89.108033</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>113.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EQI</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>123.466933</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>149.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VSD</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>100.431933</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>113.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DYC</td>\n",
       "      <td>-0.766667</td>\n",
       "      <td>127.134533</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>137.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  acid  hydrophobicity        mass  number_of_atoms      volume\n",
       "0  RQG       -2.800000  113.788733        18.666667  125.766667\n",
       "1  GPL        0.600000   89.108033        16.333333  113.166667\n",
       "2  EQI       -0.833333  123.466933        20.333333  149.633333\n",
       "3  VSD       -0.033333  100.431933        16.333333  113.366667\n",
       "4  DYC       -0.766667  127.134533        18.000000  137.733333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'data/acid_properties.csv'\n",
    "props = pd.read_csv(filename)\n",
    "\n",
    "def acid_dict(some_c, props):\n",
    "    prop_by_letter = [props[props.acid == let].iloc[:, 1:] for let in some_c]   \n",
    "    df_concat = pd.concat(prop_by_letter)\n",
    "    res = df_concat.mean()\n",
    "    dres = dict(res)\n",
    "    dres['acid'] = some_c\n",
    "    return dres\n",
    "\n",
    "save_path = 'data/all_acid_dicts.pickle'\n",
    "producer = lambda: [acid_dict(some_c, props) for some_c in tsne_df.codone]\n",
    "all_acid_dicts = read_or_create(save_path, producer)\n",
    "\n",
    "all_acid_df = pd.DataFrame(all_acid_dicts)\n",
    "all_acid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_conn(input_tensor, from_n, to_n, layer_name, activ_func=tf.nn.tanh):\n",
    "    from_n = int(from_n)\n",
    "    to_n = int(to_n)\n",
    "\n",
    "    with tf.name_scope(layer_name):\n",
    "        W_fc = tf.Variable(tf.truncated_normal(shape=(from_n, to_n), stddev=0.1), name=layer_name + '_W')\n",
    "        b_fc = tf.Variable(tf.constant(0.1, shape=(to_n,)), name=layer_name + '_b')\n",
    "        h_fc = activ_func(tf.add(tf.matmul(input_tensor, W_fc), b_fc))\n",
    "        return h_fc\n",
    "\n",
    "\n",
    "def create_fully_conn_logit(input_tensor, from_n, to_n, layer_name):\n",
    "    from_n = int(from_n)\n",
    "    to_n = int(to_n)\n",
    "\n",
    "    with tf.name_scope(layer_name):\n",
    "        W_fc = tf.Variable(tf.truncated_normal(shape=(from_n, to_n), stddev=0.1), name=layer_name + '_W')\n",
    "        b_fc = tf.Variable(tf.constant(0.1, shape=(to_n,)), name=layer_name + '_b')\n",
    "        h_fc = tf.add(tf.matmul(input_tensor, W_fc), b_fc)\n",
    "        return h_fc    \n",
    "\n",
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        self.saver = tf.train.Saver()  # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name='center_words')\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1], name='target_words')\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        with tf.name_scope(\"embed\"):\n",
    "            base_var = tf.Variable(tf.truncated_normal([self.vocab_size,\n",
    "                                                               self.embed_size], stddev=0.1),\n",
    "                                            name='embed_matrix')\n",
    "            \n",
    "            base_ratio = 1.\n",
    "            fc1_ratio = 1.5\n",
    "            fc2_ratio = 2.\n",
    "            fc3_ratio = 2.5\n",
    "            \n",
    "            #DenseNet little brother\n",
    "            fc1 = create_fully_conn(base_var, self.embed_size, self.embed_size * fc1_ratio,\n",
    "                                          'fc1', tf.nn.elu)\n",
    "            fc2 = create_fully_conn(fc1, self.embed_size * fc1_ratio, self.embed_size * fc2_ratio, \n",
    "                                          'fc2', tf.nn.sigmoid)\n",
    "            \n",
    "            con_12 = tf.concat([fc1, fc2], axis=1)\n",
    "            con_12_size = self.embed_size*(fc1_ratio + fc2_ratio)\n",
    "            \n",
    "            \n",
    "            fc3 = create_fully_conn(con_12, con_12_size, self.embed_size * fc3_ratio,\n",
    "                                    'fc3', tf.nn.relu)\n",
    "            \n",
    "            con_23 = tf.concat([fc2, fc3], axis=1)\n",
    "            con_23_size = self.embed_size*(fc3_ratio + fc2_ratio)\n",
    "            \n",
    "            self.embed_matrix = create_fully_conn_logit(con_23, con_23_size, self.embed_size, 'em')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "\n",
    "            # construct variables for NCE loss\n",
    "            nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
    "                                                         stddev=1.0 / (self.embed_size ** 0.5)),\n",
    "                                     name='nce_weight')\n",
    "            nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "\n",
    "            # define loss function to be NCE loss function\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                      biases=nce_bias,\n",
    "                                                      labels=self.target_words,\n",
    "                                                      inputs=embed,\n",
    "                                                      num_sampled=self.num_sampled,\n",
    "                                                      num_classes=self.vocab_size), name='loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "            # because you have several summaries, we should merge them all\n",
    "            # into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name histogram loss is illegal; using histogram_loss instead.\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 9424\n",
    "EMBED_SIZE = 100  # dimension of the word embedding vectors\n",
    "NUM_SAMPLED = 16  # Number of negative examples to sample.\n",
    "LEARNING_RATE = .001\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "SKIP_STEP = 2000\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_gen, num_train_steps, learning_rate, skip_step):\n",
    "    make_dir('checkpoints')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\n",
    "    \n",
    "    with tf.Session(graph=g, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "#         if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and os.path.isfile(ckpt.model_checkpoint_path):\n",
    "            model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0  # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('improved_graph/lr' + str(learning_rate), sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        \n",
    "        for index in range(initial_step, initial_step + num_train_steps):\n",
    "            centers, targets = next(batch_gen)\n",
    "#             print(centers[:5], targets[:5])\n",
    "            feed_dict = {model.center_words: centers, model.target_words: targets}\n",
    "            loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op],\n",
    "                                              feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, global_step=index)\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % skip_step == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / skip_step))\n",
    "                total_loss = 0.0\n",
    "                model.saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "\n",
    "        final_embed_matrix = sess.run(model.embed_matrix)\n",
    "        return final_embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1999:  35.8\n",
      "Average loss at step 3999:  17.9\n",
      "Average loss at step 5999:  11.8\n",
      "Average loss at step 7999:   8.8\n",
      "Average loss at step 9999:   7.2\n",
      "Average loss at step 11999:   5.7\n",
      "Average loss at step 13999:   5.1\n",
      "Average loss at step 15999:   4.6\n",
      "Average loss at step 17999:   4.3\n",
      "Average loss at step 19999:   4.1\n",
      "Average loss at step 21999:   3.9\n",
      "Average loss at step 23999:   3.7\n",
      "Average loss at step 25999:   3.7\n"
     ]
    }
   ],
   "source": [
    "final_embed_matrix = train_model(model, batch_gen, 30000, LEARNING_RATE, SKIP_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tsne_df(matrix):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    XX = tsne.fit_transform(matrix)\n",
    "    \n",
    "    tsne_df = pd.DataFrame(XX, columns=['x0', 'x1'])\n",
    "    unique_codones = sorted(dictionary, key=dictionary.get)\n",
    "    tsne_df['codone'] = list(unique_codones)\n",
    "#     tsne_df.head()\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title('unlabeled encoding', fontsize=20)\n",
    "    plt.scatter(tsne_df.x0, tsne_df.x1, s=10)\n",
    "    plt.show()\n",
    "    \n",
    "    return tsne_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_embedding_properties(final_df):\n",
    "    plt.figure(figsize=(25, 20))\n",
    "    for i, p in enumerate(['hydrophobicity', 'mass', 'number_of_atoms', 'volume']):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.title(p, fontsize=25)\n",
    "        plt.scatter(final_df.x0, final_df.x1, c=final_df[p], s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = plot_tsne_df(final_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = all_acid_df.join(tsne_df.set_index('codone'), on='acid')\n",
    "# final_df.head()\n",
    "plot_embedding_properties(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/nice_embed_tsne.csv'\n",
    "gensim_tsne_df = pd.read_csv(filename, index_col=0)\n",
    "gensim_tsne_df.columns = ['x0', 'x1', 'codone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_tsne_df(gensim_tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df_nice = all_acid_df.join(gensim_tsne_df.set_index('codone'), on='acid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_embedding_properties(final_df_nice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Improve SkipGramModel to archive better embedding for amino acids codones. Visualize your space in the similar style as on the bottom example. You are only allowed to use vanilla tensorflow for this task.\n",
    "\n",
    "Bonus task(no credit): visualize your embedding space in similar manner as minst example: https://www.tensorflow.org/versions/r0.12/how_tos/embedding_viz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
