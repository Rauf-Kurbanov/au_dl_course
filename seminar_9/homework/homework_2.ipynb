{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "\n",
    "class ImdbMovieReviews:\n",
    "    DEFAULT_URL = \\\n",
    "        'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    TOKEN_REGEX = re.compile(r'[A-Za-z]+|[!?.:,()]')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_dir = './imdb'\n",
    "        self._url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "        \n",
    "        if not os.path.isfile(self._cache_dir):\n",
    "            urllib.request.urlretrieve(self._url, self._cache_dir)\n",
    "        self.filepath = self._cache_dir\n",
    "\n",
    "    def __iter__(self):\n",
    "        with tarfile.open(self.filepath) as archive:\n",
    "            items = archive.getnames()\n",
    "            for filename in archive.getnames():\n",
    "                if filename.startswith('aclImdb/train/pos/'):\n",
    "                    yield self._read(archive, filename), True\n",
    "                elif filename.startswith('aclImdb/train/neg/'):\n",
    "                    yield self._read(archive, filename), False\n",
    "                    \n",
    "    def _read(self, archive, filename):\n",
    "        with archive.extractfile(filename) as file_:\n",
    "            data = file_.read().decode('utf-8')\n",
    "            data = type(self).TOKEN_REGEX.findall(data)\n",
    "            data = [x.lower() for x in data]\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Spacy is my favourite nlp framework, which havu builtin word embeddings trains on wikipesia\n",
    "from spacy.en import English\n",
    "\n",
    "class Embedding:\n",
    "    \n",
    "    def __init__(self):\n",
    "#          spaCy makes using word vectors very easy. \n",
    "#             The Lexeme , Token , Span  and Doc  classes all have a .vector property,\n",
    "#             which is a 1-dimensional numpy array of 32-bit floats:\n",
    "#         self.parser = spacy.load('en_vectors_web_lg')\n",
    "        self.parser = English()\n",
    "#         self._length = length\n",
    "        self.dimensions = 300\n",
    "        \n",
    "    def __call__(self, sequence, length):\n",
    "        # DO I really need them to be equal length?\n",
    "        # Let's assume I'm not\n",
    "        data = np.zeros((length, self.dimensions))\n",
    "        # you can access known words from the parser's vocabulary\n",
    "        embedded = [self.parser.vocab[w].vector for w in sequence]\n",
    "        data[:len(sequence)] = embedded\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def preprocess_batched_split(iterator, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        batch = []\n",
    "        labelss = []\n",
    "        sentence_sizes_batch = []\n",
    "        for index in range(batch_size):\n",
    "            text, label = next(iterator)\n",
    "            sents = [list(y) for x, y in itertools.groupby(text, lambda z: z == '.') if not x]\n",
    "            sentence_sizes = [len(s) for s in sents]\n",
    "            text_embed = [embedding(sent) for sent in sents]\n",
    "            \n",
    "            batch.append(text_embed)\n",
    "            labelss.append(label)\n",
    "            sentence_sizes_batch.append(sentence_sizes)\n",
    "            \n",
    "        labels_batch = np.array(labelss, dtype=np.int32)\n",
    "        sent_per_doc = np.array([len(x) for x in sentence_sizes_batch])\n",
    "        words_per_sent_per_doc = np.array(sentence_sizes_batch)\n",
    "        yield np.array(batch), labels_batch, words_per_sent_per_doc, sent_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def preprocess_batched_split2(iterator, embedding, batch_size):\n",
    "    iterator = iter(iterator)\n",
    "    while True:\n",
    "        batch, labels_b = zip(*itertools.islice(iterator, batch_size))\n",
    "        \n",
    "        sents_b = [[list(y) for x, y in itertools.groupby(doc, lambda z: z == '.') if not x] for doc in batch]\n",
    "\n",
    "        sentence_sizes_b = [[len(sent) for sent in doc] for doc in sents_b]\n",
    "        sentence_size = max(map(max, sentence_sizes_b))\n",
    "        \n",
    "        document_sizes = np.array([len(doc) for doc in sentence_sizes_b], dtype=np.int32)\n",
    "        document_size = document_sizes.max()\n",
    "\n",
    "        sentence_sizes_np = np.zeros(shape=[batch_size, document_size], dtype=np.int32)\n",
    "        for bi, ds, ss in zip(range(sentence_sizes_np.shape[0]), document_sizes, sentence_sizes_b):\n",
    "            sentence_sizes_np[bi][:ds] = ss\n",
    "        \n",
    "        text_embed_b = np.zeros((batch_size, document_size, sentence_size, 300))\n",
    "        for i, ds, doc_sents in zip(range(text_embed_b.shape[0]), document_sizes, sents_b):\n",
    "            doc_sents_embed = np.array([embedding(sent, sentence_size) for sent in doc_sents])\n",
    "            text_embed_b[i][:ds] = doc_sents_embed\n",
    "        \n",
    "        yield text_embed_b, np.array(labels_b, dtype=np.int32), np.array(document_sizes), sentence_sizes_np, sents_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = list(ImdbMovieReviews())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import HanSequenceLabellingModel, model_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 1\n",
    "# %aimport HanSequenceLabellingModel, model_components\n",
    "# %aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches_split = preprocess_batched_split2(reviews, Embedding(), batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(HanSequenceLabellingModel)\n",
    "from HanSequenceLabellingModel import HanSequenceLabellingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HAN_model_1(session, restore_only=False):\n",
    "    \"\"\"Hierarhical Attention Network\"\"\"\n",
    "    import tensorflow as tf\n",
    "    try:\n",
    "        from tensorflow.contrib.rnn import GRUCell, MultiRNNCell, DropoutWrapper\n",
    "    except ImportError:\n",
    "        MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\n",
    "        GRUCell = tf.nn.rnn_cell.GRUCell\n",
    "    from bn_lstm import BNLSTMCell\n",
    "    from HanSequenceLabellingModel import HanSequenceLabellingModel\n",
    "\n",
    "    is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "    cell = BNLSTMCell(80, is_training) # h-h batchnorm LSTMCell\n",
    "    cell = MultiRNNCell([cell]*5)\n",
    "\n",
    "    model = HanSequenceLabellingModel(\n",
    "            embedding_size=300,\n",
    "            classes=2,\n",
    "            word_cell=cell,\n",
    "            sentence_cell=cell,\n",
    "            word_output_size=300,\n",
    "            sentence_output_size=300,\n",
    "            learning_rate=0.001,\n",
    "            max_grad_norm=5.0,\n",
    "            dropout_keep_proba=0.5,\n",
    "            is_training=is_training,\n",
    "    )\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    checkpoint_dir = 'checkpoints_old'\n",
    "    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if checkpoint:\n",
    "        print(\"Reading model parameters from %s\" % checkpoint.model_checkpoint_path)\n",
    "        saver.restore(session, checkpoint.model_checkpoint_path)\n",
    "    elif restore_only:\n",
    "        raise FileNotFoundError(\"Cannot restore model\")\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "    return model, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iters = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = False\n",
    "if do_train:\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)\n",
    "\n",
    "    with tf.Session(config=config) as s:\n",
    "        model, saver = HAN_model_1(s)\n",
    "        tflog_dir = 'tf_logs'\n",
    "        summary_writer = tf.summary.FileWriter(tflog_dir, graph=tf.get_default_graph())\n",
    "\n",
    "        for i, (data, labels_batch, sent_per_doc, words_per_sent_per_doc,_) in enumerate(batches_split):\n",
    "\n",
    "            fd = {\n",
    "                model.is_training: True,\n",
    "                model.inputs_embedded: data,\n",
    "                model.word_lengths: words_per_sent_per_doc,\n",
    "                model.sentence_lengths: sent_per_doc,\n",
    "                model.labels: labels_batch,\n",
    "                model.sample_weights: np.ones(shape=(10))\n",
    "            }\n",
    "\n",
    "            t0 = time.clock()\n",
    "            step, summaries, loss, accuracy, _ = s.run([\n",
    "                    model.global_step,\n",
    "                    model.summary,\n",
    "                    model.loss,\n",
    "                    model.accuracy,\n",
    "                    model.train_op,\n",
    "            ], feed_dict=fd)\n",
    "            td = time.clock() - t0\n",
    "\n",
    "            summary_writer.add_summary(summaries, global_step=step)\n",
    "\n",
    "            checkpoint_frequency = 100\n",
    "            eval_frequency = 1\n",
    "\n",
    "            if i >= max_iters:\n",
    "                break\n",
    "\n",
    "            if step % 1 == 0:\n",
    "                print('step %s, loss=%s, accuracy=%s, t=%s, inputs=%s' % (step, loss, accuracy, round(td, 2), fd[model.inputs_embedded].shape), end='\\r')\n",
    "            if step != 0 and step % checkpoint_frequency == 0:\n",
    "    #             print('checkpoint & graph meta')\n",
    "                checkpoint_path = 'checkpoints/checkpoint'\n",
    "                saver.save(s, checkpoint_path, global_step=step)\n",
    "    #             print('checkpoint done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copied from https://gist.github.com/christian-oudard/220521\n",
    "\n",
    "# System color name constants.\n",
    "(\n",
    "    BLACK,\n",
    "    RED,\n",
    "    GREEN,\n",
    "    YELLOW,\n",
    "    BLUE,\n",
    "    MAGENTA,\n",
    "    CYAN,\n",
    "    LIGHT_GRAY,\n",
    "    DARK_GRAY,\n",
    "    BRIGHT_RED,\n",
    "    BRIGHT_GREEN,\n",
    "    BRIGHT_YELLOW,\n",
    "    BRIGHT_BLUE,\n",
    "    BRIGHT_MAGENTA,\n",
    "    BRIGHT_CYAN,\n",
    "    WHITE,\n",
    ") = range(16)\n",
    "\n",
    "def rgb(red, green, blue):\n",
    "    \"\"\"\n",
    "    Calculate the palette index of a color in the 6x6x6 color cube.\n",
    "    The red, green and blue arguments may range from 0 to 5.\n",
    "    \"\"\"\n",
    "    return 16 + (red * 36) + (green * 6) + blue\n",
    "\n",
    "def gray(value):\n",
    "    \"\"\"\n",
    "    Calculate the palette index of a color in the grayscale ramp.\n",
    "    The value argument may range from 0 to 23.\n",
    "    \"\"\"\n",
    "    return 232 + value\n",
    "\n",
    "def set_color(fg=None, bg=None):\n",
    "    \"\"\"\n",
    "    Print escape codes to set the terminal color.\n",
    "    fg and bg are indices into the color palette for the foreground and\n",
    "    background colors.\n",
    "    \"\"\"\n",
    "    print(_set_color(fg, bg), end='')\n",
    "\n",
    "def _set_color(fg=None, bg=None):\n",
    "    result = ''\n",
    "    if fg:\n",
    "        result += '\\x1b[38;5;%dm' % fg\n",
    "    if bg:\n",
    "        result += '\\x1b[48;5;%dm' % bg\n",
    "    return result\n",
    "\n",
    "def reset_color():\n",
    "    \"\"\"\n",
    "    Reset terminal color to default.\n",
    "    \"\"\"\n",
    "    print(_reset_color(), end='')\n",
    "\n",
    "def _reset_color():\n",
    "    return '\\x1b[0m'\n",
    "\n",
    "def print_color(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Print function, with extra arguments fg and bg to set colors.\n",
    "    \"\"\"\n",
    "    fg = kwargs.pop('fg', None)\n",
    "    bg = kwargs.pop('bg', None)\n",
    "    set_color(fg, bg)\n",
    "    print(*args, **kwargs)\n",
    "    reset_color()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoints_old/checkpoint-2400\n",
      "INFO:tensorflow:Restoring parameters from checkpoints_old/checkpoint-2400\n",
      "\u001b[48;5;198mthis \u001b[0m\u001b[48;5;198mmovie \u001b[0m\u001b[48;5;199mhas \u001b[0m\u001b[48;5;199mone \u001b[0m\u001b[48;5;200mredeeming \u001b[0m\u001b[48;5;199mfeature \u001b[0m\n",
      "\u001b[48;5;201mat \u001b[0m\u001b[48;5;201mone \u001b[0m\u001b[48;5;201mpoint \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mafter \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mcharacter \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201mattacked \u001b[0m\u001b[48;5;201mby \u001b[0m\u001b[48;5;201man \u001b[0m\u001b[48;5;201max \u001b[0m\u001b[48;5;201mwielding \u001b[0m\u001b[48;5;201mfairy \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mhis \u001b[0m\u001b[48;5;201mbrother \u001b[0m\u001b[48;5;201masks \u001b[0m\u001b[48;5;201mhim \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mwhy \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201myour \u001b[0m\u001b[48;5;201mdick \u001b[0m\u001b[48;5;201mover \u001b[0m\u001b[48;5;201mthere \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mchuck \u001b[0m\u001b[48;5;200m? \u001b[0m\u001b[48;5;201mafter \u001b[0m\u001b[48;5;201msuffering \u001b[0m\u001b[48;5;201mthrough \u001b[0m\u001b[48;5;201malmost \u001b[0m\u001b[48;5;201man \u001b[0m\u001b[48;5;201mhour \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;199mbad \u001b[0m\u001b[48;5;200mfilm \u001b[0m\u001b[48;5;200m, \u001b[0m\u001b[48;5;200mthis \u001b[0m\u001b[48;5;200malmost \u001b[0m\u001b[48;5;200mmade \u001b[0m\u001b[48;5;201mmy \u001b[0m\u001b[48;5;201mdrink \u001b[0m\u001b[48;5;201mcome \u001b[0m\u001b[48;5;201mout \u001b[0m\u001b[48;5;201mmy \u001b[0m\u001b[48;5;201mnose \u001b[0m\n",
      "\u001b[48;5;200mbr \u001b[0m\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201msome \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;200mthe \u001b[0m\u001b[48;5;200macting \u001b[0m\u001b[48;5;200misn \u001b[0m\u001b[48;5;200mt \u001b[0m\u001b[48;5;200mtoo \u001b[0m\u001b[48;5;199mbad \u001b[0m\u001b[48;5;199m, \u001b[0m\u001b[48;5;199mbut \u001b[0m\u001b[48;5;200mthe \u001b[0m\u001b[48;5;200mkids \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mstink \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mp \u001b[0m\n",
      "\u001b[48;5;200mj \u001b[0m\n",
      "\u001b[48;5;199msoles \u001b[0m\u001b[48;5;200mshould \u001b[0m\u001b[48;5;200mbe \u001b[0m\u001b[48;5;200mashamed \u001b[0m\u001b[48;5;200mof \u001b[0m\u001b[48;5;200mherself \u001b[0m\u001b[48;5;200mfor \u001b[0m\u001b[48;5;200mdoing \u001b[0m\u001b[48;5;200mthis \u001b[0m\u001b[48;5;199mfilm \u001b[0m\n",
      "\u001b[48;5;200mthe \u001b[0m\u001b[48;5;199mstory \u001b[0m\u001b[48;5;197mis \u001b[0m\u001b[48;5;196mweak \u001b[0m\u001b[48;5;199mand \u001b[0m\u001b[48;5;199mnobody \u001b[0m\u001b[48;5;200mdoes \u001b[0m\u001b[48;5;200mwhat \u001b[0m\u001b[48;5;200myou \u001b[0m\u001b[48;5;201mthink \u001b[0m\u001b[48;5;201m( \u001b[0m\u001b[48;5;201mor \u001b[0m\u001b[48;5;201mwhat \u001b[0m\u001b[48;5;201mcommon \u001b[0m\u001b[48;5;201msense \u001b[0m\u001b[48;5;201mdictates \u001b[0m\u001b[48;5;201m) \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mshould \u001b[0m\n",
      "\u001b[48;5;200mbr \u001b[0m\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;200mcourse \u001b[0m\u001b[48;5;200m, \u001b[0m\u001b[48;5;200mthere \u001b[0m\u001b[48;5;201mare \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mlot \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;200mstory \u001b[0m\u001b[48;5;200mpoints \u001b[0m\u001b[48;5;200mthat \u001b[0m\u001b[48;5;200mdon \u001b[0m\u001b[48;5;200mt \u001b[0m\u001b[48;5;200madd \u001b[0m\u001b[48;5;200mup \u001b[0m\n",
      "\u001b[48;5;201mfor \u001b[0m\u001b[48;5;201mexample \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201min \u001b[0m\u001b[48;5;201mone \u001b[0m\u001b[48;5;201mscene \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mghosts \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201myoung \u001b[0m\u001b[48;5;201mchildren \u001b[0m\u001b[48;5;201mmust \u001b[0m\u001b[48;5;201mconcentrate \u001b[0m\u001b[48;5;201mhard \u001b[0m\u001b[48;5;201mto \u001b[0m\u001b[48;5;201mmove \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mphysical \u001b[0m\u001b[48;5;201mobject \u001b[0m\u001b[48;5;201mso \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mprove \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mexist \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mdifficult \u001b[0m\u001b[48;5;201mfeat \u001b[0m\u001b[48;5;200msince \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mapparently \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201minteract \u001b[0m\u001b[48;5;201mwith \u001b[0m\u001b[48;5;201mphysical \u001b[0m\u001b[48;5;201mmatter \u001b[0m\n",
      "\u001b[48;5;200mhowever \u001b[0m\u001b[48;5;199m, \u001b[0m\u001b[48;5;198mminutes \u001b[0m\u001b[48;5;200mlater \u001b[0m\u001b[48;5;200mthey \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mpick \u001b[0m\u001b[48;5;201mup \u001b[0m\u001b[48;5;201mbranches \u001b[0m\u001b[48;5;201moff \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mground \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mbeat \u001b[0m\u001b[48;5;200mthe \u001b[0m\u001b[48;5;200mtooth \u001b[0m\u001b[48;5;200mfairy \u001b[0m\u001b[48;5;200mwith \u001b[0m\u001b[48;5;200mthem \u001b[0m\n",
      "\u001b[48;5;198mapparently \u001b[0m\u001b[48;5;200mthey \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201msometimes \u001b[0m\u001b[48;5;201mmove \u001b[0m\u001b[48;5;200mmatter \u001b[0m\u001b[48;5;200mand \u001b[0m\u001b[48;5;200msometimes \u001b[0m\u001b[48;5;200mthey \u001b[0m\u001b[48;5;200mcan \u001b[0m\u001b[48;5;200mt \u001b[0m\n",
      "\u001b[48;5;200mgo \u001b[0m\u001b[48;5;200mfigure \u001b[0m\n",
      "\u001b[48;5;200mbr \u001b[0m\u001b[48;5;200mbr \u001b[0m\u001b[48;5;201mlots \u001b[0m\u001b[48;5;200mof \u001b[0m\u001b[48;5;200mblood \u001b[0m\u001b[48;5;200mand \u001b[0m\u001b[48;5;200mguts \u001b[0m\u001b[48;5;200m, \u001b[0m\u001b[48;5;199mthough \u001b[0m\n",
      "\u001b[48;5;200ma \u001b[0m\u001b[48;5;200mfew \u001b[0m\u001b[48;5;200mnice \u001b[0m\u001b[48;5;199mboobs \u001b[0m\n",
      "\u001b[48;5;199mbut \u001b[0m\u001b[48;5;199mthis \u001b[0m\u001b[48;5;199mdoesn \u001b[0m\u001b[48;5;200mt \u001b[0m\u001b[48;5;200mmake \u001b[0m\u001b[48;5;200mup \u001b[0m\u001b[48;5;201mfor \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mdeficiencies \u001b[0m\n",
      "\u001b[48;5;200mbr \u001b[0m\u001b[48;5;200mbr \u001b[0m\u001b[48;5;201mif \u001b[0m\u001b[48;5;201myou \u001b[0m\u001b[48;5;201mwant \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;200mmovie \u001b[0m\u001b[48;5;200mabout \u001b[0m\u001b[48;5;200mthe \u001b[0m\u001b[48;5;200mtooth \u001b[0m\u001b[48;5;200mfairy \u001b[0m\u001b[48;5;200m, \u001b[0m\u001b[48;5;200mgo \u001b[0m\u001b[48;5;200mrent \u001b[0m\u001b[48;5;200mdarkness \u001b[0m\u001b[48;5;200mfalls \u001b[0m\n",
      "\u001b[48;5;199mi \u001b[0m\u001b[48;5;200mthink \u001b[0m\u001b[48;5;199mit \u001b[0m\u001b[48;5;200ms \u001b[0m\u001b[48;5;199mgreat \u001b[0m\u001b[48;5;200m, \u001b[0m\u001b[48;5;200mthough \u001b[0m\u001b[48;5;200ma \u001b[0m\u001b[48;5;200mlot \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mother \u001b[0m\u001b[48;5;201mreviewers \u001b[0m\u001b[48;5;201mdon \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201mshare \u001b[0m\u001b[48;5;201mmy \u001b[0m\u001b[48;5;201mopinion \u001b[0m\n",
      "\u001b[48;5;200mat \u001b[0m\u001b[48;5;200mleast \u001b[0m\u001b[48;5;199mit \u001b[0m\u001b[48;5;200msets \u001b[0m\u001b[48;5;200ma \u001b[0m\u001b[48;5;199mmood \u001b[0m\n",
      "\n",
      "\u001b[48;5;201mthis \u001b[0m\u001b[48;5;201mmovie \u001b[0m\u001b[48;5;201mhas \u001b[0m\u001b[48;5;201mone \u001b[0m\u001b[48;5;201mredeeming \u001b[0m\u001b[48;5;201mfeature \u001b[0m\n",
      "\u001b[48;5;201mat \u001b[0m\u001b[48;5;201mone \u001b[0m\u001b[48;5;201mpoint \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mafter \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mcharacter \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201mattacked \u001b[0m\u001b[48;5;201mby \u001b[0m\u001b[48;5;201man \u001b[0m\u001b[48;5;201max \u001b[0m\u001b[48;5;201mwielding \u001b[0m\u001b[48;5;201mfairy \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mhis \u001b[0m\u001b[48;5;201mbrother \u001b[0m\u001b[48;5;201masks \u001b[0m\u001b[48;5;201mhim \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mwhy \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201myour \u001b[0m\u001b[48;5;201mdick \u001b[0m\u001b[48;5;201mover \u001b[0m\u001b[48;5;201mthere \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mchuck \u001b[0m\u001b[48;5;201m? \u001b[0m\u001b[48;5;201mafter \u001b[0m\u001b[48;5;201msuffering \u001b[0m\u001b[48;5;201mthrough \u001b[0m\u001b[48;5;201malmost \u001b[0m\u001b[48;5;201man \u001b[0m\u001b[48;5;201mhour \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mbad \u001b[0m\u001b[48;5;201mfilm \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mthis \u001b[0m\u001b[48;5;201malmost \u001b[0m\u001b[48;5;201mmade \u001b[0m\u001b[48;5;201mmy \u001b[0m\u001b[48;5;201mdrink \u001b[0m\u001b[48;5;201mcome \u001b[0m\u001b[48;5;201mout \u001b[0m\u001b[48;5;201mmy \u001b[0m\u001b[48;5;201mnose \u001b[0m\n",
      "\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201msome \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201macting \u001b[0m\u001b[48;5;201misn \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201mtoo \u001b[0m\u001b[48;5;201mbad \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mbut \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mkids \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mstink \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mp \u001b[0m\n",
      "\u001b[48;5;201mj \u001b[0m\n",
      "\u001b[48;5;201msoles \u001b[0m\u001b[48;5;201mshould \u001b[0m\u001b[48;5;201mbe \u001b[0m\u001b[48;5;201mashamed \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mherself \u001b[0m\u001b[48;5;201mfor \u001b[0m\u001b[48;5;201mdoing \u001b[0m\u001b[48;5;201mthis \u001b[0m\u001b[48;5;201mfilm \u001b[0m\n",
      "\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mstory \u001b[0m\u001b[48;5;201mis \u001b[0m\u001b[48;5;201mweak \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mnobody \u001b[0m\u001b[48;5;201mdoes \u001b[0m\u001b[48;5;201mwhat \u001b[0m\u001b[48;5;201myou \u001b[0m\u001b[48;5;201mthink \u001b[0m\u001b[48;5;201m( \u001b[0m\u001b[48;5;201mor \u001b[0m\u001b[48;5;201mwhat \u001b[0m\u001b[48;5;201mcommon \u001b[0m\u001b[48;5;201msense \u001b[0m\u001b[48;5;201mdictates \u001b[0m\u001b[48;5;201m) \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mshould \u001b[0m\n",
      "\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mcourse \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mthere \u001b[0m\u001b[48;5;201mare \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mlot \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mstory \u001b[0m\u001b[48;5;201mpoints \u001b[0m\u001b[48;5;201mthat \u001b[0m\u001b[48;5;201mdon \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201madd \u001b[0m\u001b[48;5;201mup \u001b[0m\n",
      "\u001b[48;5;201mfor \u001b[0m\u001b[48;5;201mexample \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201min \u001b[0m\u001b[48;5;201mone \u001b[0m\u001b[48;5;201mscene \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mghosts \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201myoung \u001b[0m\u001b[48;5;201mchildren \u001b[0m\u001b[48;5;201mmust \u001b[0m\u001b[48;5;201mconcentrate \u001b[0m\u001b[48;5;201mhard \u001b[0m\u001b[48;5;201mto \u001b[0m\u001b[48;5;201mmove \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mphysical \u001b[0m\u001b[48;5;201mobject \u001b[0m\u001b[48;5;201mso \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mprove \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mexist \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mdifficult \u001b[0m\u001b[48;5;201mfeat \u001b[0m\u001b[48;5;201msince \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mapparently \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201minteract \u001b[0m\u001b[48;5;201mwith \u001b[0m\u001b[48;5;201mphysical \u001b[0m\u001b[48;5;201mmatter \u001b[0m\n",
      "\u001b[48;5;201mhowever \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mminutes \u001b[0m\u001b[48;5;201mlater \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mall \u001b[0m\u001b[48;5;201mpick \u001b[0m\u001b[48;5;201mup \u001b[0m\u001b[48;5;201mbranches \u001b[0m\u001b[48;5;201moff \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mground \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mbeat \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mtooth \u001b[0m\u001b[48;5;201mfairy \u001b[0m\u001b[48;5;201mwith \u001b[0m\u001b[48;5;201mthem \u001b[0m\n",
      "\u001b[48;5;201mapparently \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201msometimes \u001b[0m\u001b[48;5;201mmove \u001b[0m\u001b[48;5;201mmatter \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201msometimes \u001b[0m\u001b[48;5;201mthey \u001b[0m\u001b[48;5;201mcan \u001b[0m\u001b[48;5;201mt \u001b[0m\n",
      "\u001b[48;5;201mgo \u001b[0m\u001b[48;5;201mfigure \u001b[0m\n",
      "\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mlots \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mblood \u001b[0m\u001b[48;5;201mand \u001b[0m\u001b[48;5;201mguts \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mthough \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mfew \u001b[0m\u001b[48;5;201mnice \u001b[0m\u001b[48;5;201mboobs \u001b[0m\n",
      "\u001b[48;5;201mbut \u001b[0m\u001b[48;5;201mthis \u001b[0m\u001b[48;5;201mdoesn \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201mmake \u001b[0m\u001b[48;5;201mup \u001b[0m\u001b[48;5;201mfor \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mdeficiencies \u001b[0m\n",
      "\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mbr \u001b[0m\u001b[48;5;201mif \u001b[0m\u001b[48;5;201myou \u001b[0m\u001b[48;5;201mwant \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mmovie \u001b[0m\u001b[48;5;201mabout \u001b[0m\u001b[48;5;201mthe \u001b[0m\u001b[48;5;201mtooth \u001b[0m\u001b[48;5;201mfairy \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mgo \u001b[0m\u001b[48;5;201mrent \u001b[0m\u001b[48;5;201mdarkness \u001b[0m\u001b[48;5;201mfalls \u001b[0m\n",
      "\u001b[48;5;201mi \u001b[0m\u001b[48;5;201mthink \u001b[0m\u001b[48;5;201mit \u001b[0m\u001b[48;5;201ms \u001b[0m\u001b[48;5;201mgreat \u001b[0m\u001b[48;5;201m, \u001b[0m\u001b[48;5;201mthough \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mlot \u001b[0m\u001b[48;5;201mof \u001b[0m\u001b[48;5;201mother \u001b[0m\u001b[48;5;201mreviewers \u001b[0m\u001b[48;5;201mdon \u001b[0m\u001b[48;5;201mt \u001b[0m\u001b[48;5;201mshare \u001b[0m\u001b[48;5;201mmy \u001b[0m\u001b[48;5;201mopinion \u001b[0m\n",
      "\u001b[48;5;201mat \u001b[0m\u001b[48;5;201mleast \u001b[0m\u001b[48;5;201mit \u001b[0m\u001b[48;5;201msets \u001b[0m\u001b[48;5;201ma \u001b[0m\u001b[48;5;201mmood \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "with tf.Session(config=config) as s:\n",
    "    model, saver = HAN_model_1(s)\n",
    "    tflog_dir = 'tf_logs'\n",
    "    summary_writer = tf.summary.FileWriter(tflog_dir, graph=tf.get_default_graph())\n",
    "    \n",
    "    data, labels_batch, sent_per_doc,\\\n",
    "    words_per_sent_per_doc, sents_batch = next(batches_split)\n",
    "\n",
    "    fd = {\n",
    "        model.is_training: True,\n",
    "        model.inputs_embedded: data,\n",
    "        model.word_lengths: words_per_sent_per_doc,\n",
    "        model.sentence_lengths: sent_per_doc,\n",
    "        model.labels: labels_batch,\n",
    "        model.sample_weights: np.ones(shape=(10))\n",
    "    }\n",
    "\n",
    "    word_attentions, sentence_att = s.run([model.word_attentions, \n",
    "                                              model.sentence_attentions], \n",
    "                                             feed_dict=fd)\n",
    "    \n",
    "    sent_atts = sentence_att[0]\n",
    "    sents = sents_batch[0]\n",
    "    \n",
    "    max_sent_att = 0\n",
    "    max_word_att = 0\n",
    "    \n",
    "    for sent_index in range(len(sents)):\n",
    "        max_sent_att = max(max_sent_att, sent_atts[sent_index])\n",
    "        \n",
    "        for word_index in range(len(sents[sent_index])):\n",
    "            max_word_att = max(max_word_att, word_attentions[sent_index][word_index])\n",
    "            \n",
    "    def draw_highlighted_att(max_sent_att_loc, max_word_att_loc):\n",
    "        for sent_index in range(len(sents)):\n",
    "            for word_index in range(len(sents[sent_index])):\n",
    "                intensity = 5 - int(word_attentions[sent_index][word_index] / max_word_att_loc*5)\n",
    "                print_color(sents[sent_index][word_index], bg=rgb(5, 0, intensity), end=' ') \n",
    "            print()\n",
    "    \n",
    "    draw_highlighted_att(max_sent_att, max_word_att)\n",
    "    print()\n",
    "    draw_highlighted_att(1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoints_old/checkpoint-2400\n",
      "INFO:tensorflow:Restoring parameters from checkpoints_old/checkpoint-2400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d59243af1394429a08ad6882223a0f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "good_true = list()\n",
    "good_false = list()\n",
    "bad_false = list()\n",
    "bad_true = list()\n",
    "\n",
    "#well, sometimes there is no enought gpu memory\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model, saver = HAN_model_1(sess, True)\n",
    "\n",
    "        for i, (data, labels_batch, sent_per_doc, words_per_sent_per_doc, sents_b) in tqdm.tqdm_notebook(enumerate(batches_split), total=max_iters):\n",
    "            fd = {\n",
    "                model.is_training: True,\n",
    "                model.inputs_embedded: data,\n",
    "                model.word_lengths: words_per_sent_per_doc,\n",
    "                model.sentence_lengths: sent_per_doc,\n",
    "                model.labels: labels_batch,\n",
    "                model.sample_weights: np.ones(shape=(10))\n",
    "            }\n",
    "\n",
    "            words, sentences = sess.run([model.word_attentions, model.sentence_attentions], feed_dict=fd)\n",
    "\n",
    "            for bt in range(len(sents_b)):\n",
    "                for sent_num in range(len(sents_b[bt])):\n",
    "                    for g in range(len(sents_b[bt][sent_num])):\n",
    "                        val = words[bt * len(sentences[0]) + sent_num][g]\n",
    "                        if (sents_b[bt][sent_num][g] == 'good'):\n",
    "                            if labels_batch[bt] == True:\n",
    "                                good_true.append(val)\n",
    "                            else:\n",
    "                                good_false.append(val)\n",
    "\n",
    "                        if (sents_b[bt][sent_num][g] == 'bad'):\n",
    "                            if labels_batch[bt] == True:\n",
    "                                bad_true.append(val)\n",
    "                            else:\n",
    "                                bad_false.append(val)\n",
    "\n",
    "            if (i >= max_iters):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
