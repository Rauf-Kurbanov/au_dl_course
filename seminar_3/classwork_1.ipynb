{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need tf.nn.softmax_cross_entropy_with_logits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computes softmax cross entropy between `logits` and `labels`.\n",
      "\n",
      "  Measures the probability error in discrete classification tasks in which the\n",
      "  classes are mutually exclusive (each entry is in exactly one class).  For\n",
      "  example, each CIFAR-10 image is labeled with one and only one label: an image\n",
      "  can be a dog or a truck, but not both.\n",
      "\n",
      "  **NOTE:**  While the classes are mutually exclusive, their probabilities\n",
      "  need not be.  All that is required is that each row of `labels` is\n",
      "  a valid probability distribution.  If they are not, the computation of the\n",
      "  gradient will be incorrect.\n",
      "\n",
      "  If using exclusive `labels` (wherein one and only\n",
      "  one class is true at a time), see `sparse_softmax_cross_entropy_with_logits`.\n",
      "\n",
      "  **WARNING:** This op expects unscaled logits, since it performs a `softmax`\n",
      "  on `logits` internally for efficiency.  Do not call this op with the\n",
      "  output of `softmax`, as it will produce incorrect results.\n",
      "\n",
      "  `logits` and `labels` must have the same shape, e.g.\n",
      "  `[batch_size, num_classes]` and the same dtype (either `float16`, `float32`,\n",
      "  or `float64`).\n",
      "\n",
      "  **Note that to avoid confusion, it is required to pass only named arguments to\n",
      "  this function.**\n",
      "\n",
      "  Args:\n",
      "    _sentinel: Used to prevent positional parameters. Internal, do not use.\n",
      "    labels: Each row `labels[i]` must be a valid probability distribution.\n",
      "    logits: Unscaled log probabilities.\n",
      "    dim: The class dimension. Defaulted to -1 which is the last dimension.\n",
      "    name: A name for the operation (optional).\n",
      "\n",
      "  Returns:\n",
      "    A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the\n",
      "    softmax cross entropy loss.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(tf.nn.softmax_cross_entropy_with_logits.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "\n",
    "$softmax(x) = \\frac{\\exp(x)}{\\sum_j \\exp(x_j)}$\n",
    "\n",
    "#### What do we want:\n",
    "\n",
    "$layer(x) = \\frac{\\exp(W x + b)}{\\sum_j \\exp(W x_j + b)}$\n",
    "\n",
    "#### How we did it in practice:\n",
    "\n",
    "tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "#### Why not FullyConnected + SoftMax?\n",
    "\n",
    "Numeric error!\n",
    "\n",
    "$\\sum_{i=1}^N \\log softmax_i(x_i) = \\sum_{i=1}^N \\sum_{j=1}^C [y_i = j] \\log softmax_j(x_i) =$\n",
    "\n",
    "$\\sum_{i=1}^N \\sum_{j=1}^C [y_i = j](x_{ij} - \\log \\sum_k \\exp(x_k) =$\n",
    "\n",
    "$\\sum_{i=1}^N \\sum_{j=1}^C [y_i = j](x_{ij} - \\log exp(x_{max})(\\sum_k \\exp(x_k - x_{max}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeakyRelu(x, alpha):\n",
    "    return tf.maximum(alpha*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before [ 1.75657368 -2.1488905  -3.24281931 -1.53599739 -2.89721012]\n",
      "after [ 1.75657368 -1.07444525 -1.62140965 -0.7679987  -1.44860506]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inp = tf.Variable(initial_value=tf.random_uniform(shape=[5], minval=-5, maxval=5, dtype=tf.float32))\n",
    "    alpha = 0.5\n",
    "    res = LeakyRelu(inp, alpha)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    before, after = sess.run([inp, res])\n",
    "    print('before', before)\n",
    "    print('after', after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PRelu(x):\n",
    "    alpha = tf.Variable(initial_value=tf.random_normal(shape=x.shape))\n",
    "    return tf.where(x < 0, alpha * x, tf.nn.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before [-0.50967312  0.73188877 -2.30100393  4.94852257  3.54626083]\n",
      "after [ 0.12812272  0.73188877  0.71911305  4.94852257  3.54626083]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    inp = tf.Variable(initial_value=tf.random_uniform(shape=[5], minval=-5, maxval=5, dtype=tf.float32))\n",
    "    alpha = 0.5\n",
    "    res = PRelu(inp)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    before, after = sess.run([inp, res])\n",
    "    print('before', before)\n",
    "    print('after', after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spp_layer(input_, levels=[2, 1], name = 'SPP_layer'):\n",
    "    '''Multiple Level SPP layer.\n",
    "       Works for levels=[1, 2, 3, 6].'''\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        pool_outputs = []\n",
    "        for l in levels:\n",
    "            pool = tf.nn.max_pool(input_, ksize=[1, np.ceil(shape[1] * 1. / l).astype(np.int32),\n",
    "                                                 np.ceil(shape[2] * 1. / l).astype(np.int32), 1], \n",
    "                                  strides=[1, np.floor(shape[1] * 1. / l + 1).astype(np.int32),\n",
    "                                           np.floor(shape[2] * 1. / l + 1), 1], \n",
    "                                  padding='SAME')\n",
    "            pool_outputs.append(tf.reshape(pool, [shape[0], -1]))\n",
    "        spp_pool = tf.concat(1, pool_outputs)\n",
    "    return spp_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spartial Pyramid Pooling coming soon\n",
    "\n",
    "PR: https://github.com/tensorflow/tensorflow/pull/12852/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
